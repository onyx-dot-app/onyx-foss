# Default values for onyx.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

global:
  # Global version for all Onyx components (overrides .Chart.AppVersion)
  version: "latest"
  # Global pull policy for all Onyx component images
  pullPolicy: "IfNotPresent"

postgresql:
  enabled: true
  cluster:
    instances: 1
    storage:
      storageClass: ""
      size: 10Gi
    enableSuperuserAccess: true
    superuserSecret:
      name: onyx-postgresql  # keep in sync with auth.postgresql

vespa:
  name: da-vespa-0
  service:
    name: vespa-service
  volumeClaimTemplates:
    - metadata:
        name: vespa-storage
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 30Gi
        storageClassName: ""
  enabled: true
  replicaCount: 1
  image:
    repository: vespa
    tag: "8.526.15"
  podAnnotations: {}
  podLabels:
    app: vespa
    app.kubernetes.io/instance: onyx
    app.kubernetes.io/name: vespa
  securityContext:
    privileged: true
    runAsUser: 0
  resources:
    # The Vespa Helm chart specifies default resources, which are quite modest. We override
    # them here to increase chances of the chart running successfully. If you plan to index at
    # scale, you will likely need to increase these limits further.
    # At large scale, it is recommended to use a dedicated Vespa cluster / Vespa cloud.
    requests:
      cpu: 4000m
      memory: 8000Mi
    limits:
      cpu: 8000m
      memory: 32000Mi

persistent:
  storageClassName: ""

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

autoscaling:
  # Valid options: 'hpa' (default) or 'keda'.
  # Set to 'keda' to render KEDA ScaledObjects for components that have autoscaling enabled.
  # When using KEDA you must install and manage the KEDA operator separately; it is not bundled with this chart.
  engine: hpa

inferenceCapability:
  service:
    portName: modelserver
    type: ClusterIP
    servicePort: 9000
    targetPort: 9000
  name: inference-model-server
  replicaCount: 1
  labels:
    - key: app
      value: inference-model-server
  image:
    repository: onyxdotapp/onyx-model-server
    # Overrides the image tag whose default is the chart appVersion.
    tag: ""
  containerPorts:
    server: 9000
  podLabels:
    - key: app
      value: inference-model-server
  resources:
    requests:
      cpu: 2000m
      memory: 3Gi
    limits:
      cpu: 4000m
      memory: 10Gi
  podSecurityContext: {}
  securityContext:
    privileged: true
    runAsUser: 0
  nodeSelector: {}

indexCapability:
  service:
    portName: modelserver
    type: ClusterIP
    servicePort: 9000
    targetPort: 9000
  replicaCount: 1
  name: indexing-model-server
  deploymentLabels:
    app: indexing-model-server
  podLabels:
    app: indexing-model-server
  indexingOnly: "True"
  podAnnotations: {}
  containerPorts:
    server: 9000
  image:
    repository: onyxdotapp/onyx-model-server
    # Overrides the image tag whose default is the chart appVersion.
    tag: ""
  limitConcurrency: 10
  resources:
    requests:
      cpu: 4000m
      memory: 3Gi
    limits:
      cpu: 6000m
      memory: 6Gi
  podSecurityContext: {}
  securityContext:
    privileged: true
    runAsUser: 0
  nodeSelector: {}
config:
  envConfigMapName: env-configmap

serviceAccount:
  # Specifies whether a service account should be created
  create: false
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

nginx:
  enabled: true
  # Nginx proxy timeout settings (in seconds)
  timeouts:
    connect: 300  # Time to establish connection with upstream server
    send: 300     # Time to send request to upstream server
    read: 300     # Time to read response from upstream server
  controller:
    containerPort:
      http: 1024

    # Propagate DOMAIN into nginx so server_name continues to use the same env var
    extraEnvs:
      - name: DOMAIN
        value: localhost

    config:
      # Expose DOMAIN to the nginx config and pull in our custom snippets
      main-snippet: |
        env DOMAIN;
      http-snippet: |
        include /etc/nginx/custom-snippets/upstreams.conf;
        include /etc/nginx/custom-snippets/server.conf;

    # Mount the existing nginx ConfigMap that holds the upstream and server snippets
    extraVolumes:
      - name: nginx-config
        configMap:
          name: onyx-nginx-conf
    extraVolumeMounts:
      - name: nginx-config
        mountPath: /etc/nginx/custom-snippets
        readOnly: true

    service:
      type: LoadBalancer
      ports:
        http: 80
      targetPorts:
        http: http

webserver:
  replicaCount: 1
  image:
    repository: onyxdotapp/onyx-web-server
    # Overrides the image tag whose default is the chart appVersion.
    tag: ""
  deploymentLabels:
    app: web-server
  podAnnotations: {}
  podLabels:
    app: web-server
  podSecurityContext:
    {}
    # fsGroup: 2000

  securityContext:
    {}
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000

  containerPorts:
    server: 3000

  service:
    type: ClusterIP
    servicePort: 3000
    targetPort: http

  resources:
    requests:
      cpu: 200m
      memory: 512Mi
    limits:
      cpu: 1000m
      memory: 1Gi

  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
    # KEDA specific configurations (only used when autoscaling.engine is set to 'keda')
    pollingInterval: 30  # seconds
    cooldownPeriod: 300  # seconds
    idleReplicaCount: 1  # minimum replicas when idle
    failureThreshold: 3  # number of failures before fallback
    fallbackReplicas: 1  # replicas to maintain on failure
    # Custom triggers for advanced KEDA configurations
    # Example: customTriggers: []
    #   - type: prometheus
    #     metadata:
    #       serverAddress: http://prometheus:9090
    #       metricName: http_requests_per_second
    #       threshold: '100'
    customTriggers: []

  # Additional volumes on the output Deployment definition.
  volumes: []
  # - name: foo
  #   secret:
  #     secretName: mysecret
  #     optional: false

  # Additional volumeMounts on the output Deployment definition.
  volumeMounts: []
  # - name: foo
  #   mountPath: "/etc/foo"
  #   readOnly: true

  nodeSelector: {}
  tolerations: []
  affinity: {}

api:
  replicaCount: 1
  image:
    repository: onyxdotapp/onyx-backend
    # Overrides the image tag whose default is the chart appVersion.
    tag: ""
  deploymentLabels:
    app: api-server
  podAnnotations: {}
  podLabels:
    scope: onyx-backend
    app: api-server

  containerPorts:
    server: 8080

  podSecurityContext:
    {}
    # fsGroup: 2000

  securityContext:
    {}
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000

  service:
    type: ClusterIP
    servicePort: 8080
    targetPort: api-server-port
    portName: api-server-port

  resources:
    requests:
      cpu: 500m
      memory: 1Gi
    limits:
      cpu: 1000m
      memory: 3Gi

  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
    # KEDA specific configurations (only used when autoscaling.engine is set to 'keda')
    pollingInterval: 30  # seconds
    cooldownPeriod: 300  # seconds
    idleReplicaCount: 1  # minimum replicas when idle
    failureThreshold: 3  # number of failures before fallback
    fallbackReplicas: 1  # replicas to maintain on failure
    # Custom triggers for advanced KEDA configurations
    # Example: customTriggers: []
    #   - type: prometheus
    #     metadata:
    #       serverAddress: http://prometheus:9090
    #       metricName: http_requests_per_second
    #       threshold: '100'
    customTriggers: []

  # Additional volumes on the output Deployment definition.
  volumes: []
  # - name: foo
  #   secret:
  #     secretName: mysecret
  #     optional: false

  # Additional volumeMounts on the output Deployment definition.
  volumeMounts: []
  # - name: foo
  #   mountPath: "/etc/foo"
  #   readOnly: true

  nodeSelector: {}
  tolerations: []


######################################################################
#
# Background workers
#
######################################################################

celery_shared:
  image:
    repository: onyxdotapp/onyx-backend
    tag: ""  # Overrides the image tag whose default is the chart appVersion.
  startupProbe:
    # startupProbe fails after 2m
    exec:
      command: ["test", "-f", "/app/onyx/main.py"]
    failureThreshold: 24
    periodSeconds: 5
    timeoutSeconds: 3
  readinessProbe:
    # readinessProbe fails after 15s + 2m of inactivity
    # it's ok to see the readinessProbe fail transiently while the container starts
    initialDelaySeconds: 15
    periodSeconds: 5
    failureThreshold: 24
    timeoutSeconds: 3
  livenessProbe:
    # livenessProbe fails after 5m of inactivity
    initialDelaySeconds: 60
    periodSeconds: 60
    failureThreshold: 5
    timeoutSeconds: 3
  podSecurityContext: {}
  securityContext:
    privileged: true
    runAsUser: 0

celery_beat:
  replicaCount: 1
  podAnnotations: {}
  podLabels:
    scope: onyx-backend-celery
    app: celery-beat
  deploymentLabels:
    app: celery-beat
  resources:
    requests:
      cpu: 500m
      memory: 512Mi
    limits:
      cpu: 1000m
      memory: 1Gi
  volumes: []  # Additional volumes on the output Deployment definition.
  volumeMounts: []  # Additional volumeMounts on the output Deployment definition.
  nodeSelector: {}
  tolerations: []
  affinity: {}

celery_worker_heavy:
  replicaCount: 1
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
    # KEDA specific configurations (only used when autoscaling.engine is set to 'keda')
    pollingInterval: 30  # seconds
    cooldownPeriod: 300  # seconds
    idleReplicaCount: 1  # minimum replicas when idle
    failureThreshold: 3  # number of failures before fallback
    fallbackReplicas: 1  # replicas to maintain on failure
    # Custom triggers for advanced KEDA configurations
    customTriggers: []
  podAnnotations: {}
  podLabels:
    scope: onyx-backend-celery
    app: celery-worker-heavy
  deploymentLabels:
    app: celery-worker-heavy
  resources:
    requests:
      cpu: 500m
      memory: 512Mi
    limits:
      cpu: 1000m
      memory: 2Gi
  volumes: []  # Additional volumes on the output Deployment definition.
  volumeMounts: []  # Additional volumeMounts on the output Deployment definition.
  nodeSelector: {}
  tolerations: []
  affinity: {}

celery_worker_docprocessing:
  replicaCount: 1
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 20
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
    # KEDA specific configurations (only used when autoscaling.engine is set to 'keda')
    pollingInterval: 30  # seconds
    cooldownPeriod: 300  # seconds
    idleReplicaCount: 1  # minimum replicas when idle
    failureThreshold: 3  # number of failures before fallback
    fallbackReplicas: 1  # replicas to maintain on failure
    # Custom triggers for advanced KEDA configurations
    customTriggers: []
  podAnnotations: {}
  podLabels:
    scope: onyx-backend-celery
    app: celery-worker-docprocessing
  deploymentLabels:
    app: celery-worker-docprocessing
  resources:
    requests:
      cpu: 500m
      memory: 2Gi
    limits:
      cpu: 1000m
      memory: 12Gi
  volumes: []  # Additional volumes on the output Deployment definition.
  volumeMounts: []  # Additional volumeMounts on the output Deployment definition.
  nodeSelector: {}
  tolerations: []
  affinity: {}

celery_worker_light:
  replicaCount: 1
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
    # KEDA specific configurations (only used when autoscaling.engine is set to 'keda')
    pollingInterval: 30  # seconds
    cooldownPeriod: 300  # seconds
    idleReplicaCount: 1  # minimum replicas when idle
    failureThreshold: 3  # number of failures before fallback
    fallbackReplicas: 1  # replicas to maintain on failure
    # Custom triggers for advanced KEDA configurations
    customTriggers: []
  podAnnotations: {}
  podLabels:
    scope: onyx-backend-celery
    app: celery-worker-light
  deploymentLabels:
    app: celery-worker-light
  resources:
    requests:
      cpu: 250m
      memory: 512Mi
    limits:
      cpu: 2000m
      memory: 4Gi
  volumes: []  # Additional volumes on the output Deployment definition.
  volumeMounts: []  # Additional volumeMounts on the output Deployment definition.
  nodeSelector: {}
  tolerations: []
  affinity: {}

celery_worker_monitoring:
  replicaCount: 1
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
    # KEDA specific configurations (only used when autoscaling.engine is set to 'keda')
    pollingInterval: 30  # seconds
    cooldownPeriod: 300  # seconds
    idleReplicaCount: 1  # minimum replicas when idle
    failureThreshold: 3  # number of failures before fallback
    fallbackReplicas: 1  # replicas to maintain on failure
    # Custom triggers for advanced KEDA configurations
    customTriggers: []
  podAnnotations: {}
  podLabels:
    scope: onyx-backend-celery
    app: celery-worker-monitoring
  deploymentLabels:
    app: celery-worker-monitoring
  resources:
    requests:
      cpu: 500m
      memory: 512Mi
    limits:
      cpu: 1000m
      memory: 4Gi
  volumes: []  # Additional volumes on the output Deployment definition.
  volumeMounts: []  # Additional volumeMounts on the output Deployment definition.
  nodeSelector: {}
  tolerations: []
  affinity: {}

celery_worker_primary:
  replicaCount: 1
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
    # KEDA specific configurations (only used when autoscaling.engine is set to 'keda')
    pollingInterval: 30  # seconds
    cooldownPeriod: 300  # seconds
    idleReplicaCount: 1  # minimum replicas when idle
    failureThreshold: 3  # number of failures before fallback
    fallbackReplicas: 1  # replicas to maintain on failure
    # Custom triggers for advanced KEDA configurations
    customTriggers: []
  podAnnotations: {}
  podLabels:
    scope: onyx-backend-celery
    app: celery-worker-primary
  deploymentLabels:
    app: celery-worker-primary
  resources:
    requests:
      cpu: 500m
      memory: 2Gi
    limits:
      cpu: 1000m
      memory: 4Gi
  volumes: []  # Additional volumes on the output Deployment definition.
  volumeMounts: []  # Additional volumeMounts on the output Deployment definition.
  nodeSelector: {}
  tolerations: []
  affinity: {}

celery_worker_user_files_indexing:
  replicaCount: 1
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
    # KEDA specific configurations (only used when autoscaling.engine is set to 'keda')
    pollingInterval: 30  # seconds
    cooldownPeriod: 300  # seconds
    idleReplicaCount: 1  # minimum replicas when idle
    failureThreshold: 3  # number of failures before fallback
    fallbackReplicas: 1  # replicas to maintain on failure
    # Custom triggers for advanced KEDA configurations
    customTriggers: []
  podAnnotations: {}
  podLabels:
    scope: onyx-backend-celery
    app: celery-worker-user-files-indexing
  deploymentLabels:
    app: celery-worker-user-files-indexing
  resources:
    requests:
      cpu: 500m
      memory: 512Mi
    limits:
      cpu: 2000m
      memory: 2Gi
  volumes: []  # Additional volumes on the output Deployment definition.
  volumeMounts: []  # Additional volumeMounts on the output Deployment definition.
  nodeSelector: {}
  tolerations: []
  affinity: {}

celery_worker_user_file_processing:
  replicaCount: 1
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
    # KEDA specific configurations
    pollingInterval: 30  # seconds
    cooldownPeriod: 300  # seconds
    idleReplicaCount: 1  # minimum replicas when idle
    failureThreshold: 3  # number of failures before fallback
    fallbackReplicas: 1  # replicas to maintain on failure
    # Custom triggers for advanced KEDA configurations
    customTriggers: []
  podAnnotations: {}
  podLabels:
    scope: onyx-backend-celery
    app: celery-worker-user-file-processing
  deploymentLabels:
    app: celery-worker-user-file-processing
  resources:
    requests:
      cpu: 500m
      memory: 512Mi
    limits:
      cpu: 2000m
      memory: 2Gi
  volumes: []  # Additional volumes on the output Deployment definition.
  volumeMounts: []  # Additional volumeMounts on the output Deployment definition.
  nodeSelector: {}
  tolerations: []
  affinity: {}

slackbot:
  enabled: true
  replicaCount: 1
  image:
    repository: onyxdotapp/onyx-backend
    tag: ""  # Overrides the image tag whose default is the chart appVersion.
  podAnnotations: {}
  podLabels:
    scope: onyx-backend
    app: slack-bot
  deploymentLabels:
    app: slack-bot
  podSecurityContext:
    {}
  securityContext:
    {}
  resources:
    requests:
      cpu: "500m"
      memory: "512Mi"
    limits:
      cpu: "1000m"
      memory: "2000Mi"
  nodeSelector: {}

celery_worker_docfetching:
  replicaCount: 1
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 20
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
    # KEDA specific configurations (only used when autoscaling.engine is set to 'keda')
    pollingInterval: 30  # seconds
    cooldownPeriod: 300  # seconds
    idleReplicaCount: 1  # minimum replicas when idle
    failureThreshold: 3  # number of failures before fallback
    fallbackReplicas: 1  # replicas to maintain on failure
    # Custom triggers for advanced KEDA configurations
    customTriggers: []
  podAnnotations: {}
  podLabels:
    scope: onyx-backend-celery
    app: celery-worker-docfetching
  deploymentLabels:
    app: celery-worker-docfetching
  resources:
    requests:
      cpu: 500m
      memory: 2Gi
    limits:
      cpu: 1000m
      memory: 16Gi
  volumes: []  # Additional volumes on the output Deployment definition.
  volumeMounts: []  # Additional volumeMounts on the output Deployment definition.
  nodeSelector: {}
  tolerations: []
  affinity: {}

######################################################################
#
# End background workers section
#
######################################################################

redis:
  enabled: true
  redisStandalone:
    image: quay.io/opstree/redis
    tag: v7.0.15
    imagePullPolicy: IfNotPresent
    serviceType: ClusterIP
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 512Mi
    # Use existing secret for Redis password
    redisSecret:
      secretName: onyx-redis
      secretKey: redis_password
  # Redis configuration
  externalConfig:
    enabled: true
    data: |
      appendonly no
      save ""
  storageSpec:
    volumeClaimTemplate:
      spec:
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 1Gi

minio:
  enabled: true
  mode: standalone
  replicas: 1
  drivesPerNode: 1
  existingSecret: onyx-objectstorage
  buckets:
    - name: onyx-file-store-bucket
  persistence:
    enabled: true
    size: 30Gi
    storageClass: ""
  service:
    type: ClusterIP
    port: 9000
  consoleService:
    type: ClusterIP
    port: 9001

ingress:
  enabled: false
  className: ""
  api:
    host: onyx.local
  webserver:
    host: onyx.local

letsencrypt:
  enabled: false
  email: "abc@abc.com"

# -- Governs all Secrets created or used by this chart. Values set by this chart will be base64 encoded in the k8s cluster.
auth:
  postgresql:
    # -- Enable or disable this secret entirely. Will remove from env var configurations and remove any created secrets.
    enabled: true
    # -- Overwrite the default secret name, ignored if existingSecret is defined
    secretName: 'onyx-postgresql'
    # -- Use a secret specified elsewhere
    existingSecret: ""
    # -- This defines the env var to secret map, key is always upper-cased as an env var
    secretKeys:
      # CloudNativePG requires `username` and `password` keys for the superuser secret.
      POSTGRES_USER: username
      POSTGRES_PASSWORD: password
    # -- Secrets values IF existingSecret is empty. Key here must match the value in secretKeys to be used. Values will be base64 encoded in the k8s cluster.
    values:
      username: "postgres"
      password: "postgres"
  redis:
    # -- Enable or disable this secret entirely. Will remove from env var configurations and remove any created secrets.
    enabled: true
    # -- Overwrite the default secret name, ignored if existingSecret is defined
    secretName: 'onyx-redis'
    # -- Use a secret specified elsewhere
    existingSecret: ""
    # -- This defines the env var to secret map, key is always upper-cased as an env var
    secretKeys:
      REDIS_PASSWORD: redis_password
    # -- Secrets values IF existingSecret is empty. Key here must match the value in secretKeys to be used. Values will be base64 encoded in the k8s cluster.
    values:
      redis_password: "password"
  objectstorage:
    # -- Enable or disable this secret entirely. Will remove from env var configurations and remove any created secrets.
    enabled: true
    # -- Overwrite the default secret name, ignored if existingSecret is defined
    secretName: 'onyx-objectstorage'
    # -- Use a secret specified elsewhere
    existingSecret: ""
    # -- This defines the env var to secret map, key is always upper-cased as an env var
    secretKeys:
      S3_AWS_ACCESS_KEY_ID: s3_aws_access_key_id
      S3_AWS_SECRET_ACCESS_KEY: s3_aws_secret_access_key
    # -- Secrets values IF existingSecret is empty. Key here must match the value in secretKeys to be used. Values will be base64 encoded in the k8s cluster.
    values:
      s3_aws_access_key_id: "minioadmin"
      s3_aws_secret_access_key: "minioadmin"
      rootUser: "minioadmin"
      rootPassword: "minioadmin"
  oauth:
    # -- Enable or disable this secret entirely. Will remove from env var configurations and remove any created secrets.
    enabled: false
    # -- Overwrite the default secret name, ignored if existingSecret is defined
    secretName: 'onyx-oauth'
    # -- Use a secret specified elsewhere
    existingSecret: ""
    # -- This defines the env var to secret map, key is always upper-cased as an env var
    secretKeys:
      OAUTH_CLIENT_ID: "oauth_client_id"
      OAUTH_CLIENT_SECRET: "oauth_client_secret"
      OAUTH_COOKIE_SECRET: "oauth_cookie_secret"
    # -- Secrets values IF existingSecret is empty. Key here must match the value in secretKeys to be used. Values will be base64 encoded in the k8s cluster.
    values:
      oauth_client_id: ""
      oauth_client_secret: ""
      oauth_cookie_secret: ""
  smtp:
    # -- Enable or disable this secret entirely. Will remove from env var configurations and remove any created secrets.
    enabled: false
    # -- Overwrite the default secret name, ignored if existingSecret is defined
    secretName: 'onyx-smtp'
    # -- Use a secret specified elsewhere
    existingSecret: ""
    # -- This defines the env var to secret map, key is always upper-cased as an env var
    secretKeys:
      SMTP_PASS: "smtp_pass"
    # -- Secrets values IF existingSecret is empty. Key here must match the value in secretKeys to be used. Values will be base64 encoded in the k8s cluster.
    values:
      smtp_pass: ""
  dbreadonly:
    # -- Enable or disable this secret entirely. Will remove from env var configurations and remove any created secrets.
    enabled: false
    # -- Overwrite the default secret name, ignored if existingSecret is defined
    secretName: 'onyx-dbreadonly'
    # -- Use a secret specified elsewhere
    existingSecret: ""
    # -- This defines the env var to secret map, key is always upper-cased as an env var
    secretKeys:
      DB_READONLY_USER: db_readonly_user
      DB_READONLY_PASSWORD: db_readonly_password
    # -- Secrets values IF existingSecret is empty. Key here must match the value in secretKeys to be used. Values will be base64 encoded in the k8s cluster.
    values:
      db_readonly_user: ""
      db_readonly_password: ""

configMap:
  # Change this for production uses unless Onyx is only accessible behind VPN
  AUTH_TYPE: "disabled"
  # 1 Day Default
  SESSION_EXPIRE_TIME_SECONDS: "86400"
  # Can be something like onyx.app, as an extra double-check
  VALID_EMAIL_DOMAINS: ""
  # For sending verification emails, true or false
  REQUIRE_EMAIL_VERIFICATION: ""
  # If unspecified then defaults to 'smtp.gmail.com'
  SMTP_SERVER: ""
  # For sending verification emails, if unspecified then defaults to '587'
  SMTP_PORT: ""
# 'your-email@company.com'
  SMTP_USER: ""
  # 'your-gmail-password'
  # SMTP_PASS: ""
  # 'your-email@company.com' SMTP_USER missing used instead
  EMAIL_FROM: ""
  # MinIO/S3 Configuration override
  S3_ENDPOINT_URL: ""  # only used if minio is not enabled
  S3_FILE_STORE_BUCKET_NAME: ""
  # Gen AI Settings
  GEN_AI_MAX_TOKENS: ""
  QA_TIMEOUT: "60"
  MAX_CHUNKS_FED_TO_CHAT: ""
  DISABLE_LLM_DOC_RELEVANCE: ""
  DISABLE_LLM_CHOOSE_SEARCH: ""
  DISABLE_LLM_QUERY_REPHRASE: ""
  # Query Options
  DOC_TIME_DECAY: ""
  HYBRID_ALPHA: ""
  EDIT_KEYWORD_QUERY: ""
  MULTILINGUAL_QUERY_EXPANSION: ""
  LANGUAGE_HINT: ""
  LANGUAGE_CHAT_NAMING_HINT: ""
  # Internet Search Tool
  BING_API_KEY: ""
  EXA_API_KEY: ""
  # Don't change the NLP models unless you know what you're doing
  EMBEDDING_BATCH_SIZE: ""
  DOCUMENT_ENCODER_MODEL: ""
  NORMALIZE_EMBEDDINGS: ""
  ASYM_QUERY_PREFIX: ""
  ASYM_PASSAGE_PREFIX: ""
  DISABLE_RERANK_FOR_STREAMING: ""
  MODEL_SERVER_PORT: ""
  MIN_THREADS_ML_MODELS: ""
  # Indexing Configs
  VESPA_SEARCHER_THREADS: ""
  NUM_INDEXING_WORKERS: ""
  DISABLE_INDEX_UPDATE_ON_SWAP: ""
  DASK_JOB_CLIENT_ENABLED: ""
  CONTINUE_ON_CONNECTOR_FAILURE: ""
  EXPERIMENTAL_CHECKPOINTING_ENABLED: ""
  CONFLUENCE_CONNECTOR_LABELS_TO_SKIP: ""
  JIRA_CLOUD_API_VERSION: ""
  JIRA_SERVER_API_VERSION: ""
  GONG_CONNECTOR_START_TIME: ""
  NOTION_CONNECTOR_ENABLE_RECURSIVE_PAGE_LOOKUP: ""
  # Worker Parallelism
  CELERY_WORKER_DOCPROCESSING_CONCURRENCY: ""
  CELERY_WORKER_LIGHT_CONCURRENCY: ""
  CELERY_WORKER_LIGHT_PREFETCH_MULTIPLIER: ""
  CELERY_WORKER_USER_FILE_PROCESSING_CONCURRENCY: ""
  # OnyxBot SlackBot Configs
  ONYX_BOT_DISABLE_DOCS_ONLY_ANSWER: ""
  ONYX_BOT_DISPLAY_ERROR_MSGS: ""
  ONYX_BOT_RESPOND_EVERY_CHANNEL: ""
  NOTIFY_SLACKBOT_NO_ANSWER: ""
  # Logging
  # Optional Telemetry, please keep it on (nothing sensitive is collected)? <3
  DISABLE_TELEMETRY: ""
  LOG_LEVEL: ""
  LOG_ALL_MODEL_INTERACTIONS: ""
  LOG_ONYX_MODEL_INTERACTIONS: ""
  LOG_VESPA_TIMING_INFORMATION: ""
  # Shared or Non-backend Related
  WEB_DOMAIN: "http://localhost:3000"
  # DOMAIN used by nginx
  DOMAIN: "localhost"
  # Chat Configs
  HARD_DELETE_CHATS: ""
